"""
ì»¤ë§¨ë“œ ì„¼í„° - ì „ì²´ ì‹œìŠ¤í…œ í†µí•© ë° ì˜ì‚¬ê²°ì •
ê°•í™”í•™ìŠµ ê¸°ë°˜ ë§¤ë§¤ ì˜ì‚¬ê²°ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import logging
from typing import Dict, List, Optional
from api import KISApi
from strategy import TradeHistory
from .market_state import MarketState
from .rl_agent import RLAgent


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CommandCenter:
    """ì»¤ë§¨ë“œ ì„¼í„° - AI ê¸°ë°˜ ë§¤ë§¤ ì˜ì‚¬ê²°ì •"""

    def __init__(self, api: KISApi, trade_history: TradeHistory):
        self.api = api
        self.trade_history = trade_history

        # ì‹œì¥ ìƒíƒœ ë¶„ì„ê¸°
        self.market_state = MarketState(api, trade_history)

        # ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸
        self.rl_agent = RLAgent(
            state_size=10,
            n_actions=5,
            learning_rate=0.1,
            discount_factor=0.95,
            epsilon=0.1,
        )

        # ì´ì „ ìƒíƒœ ë° í–‰ë™ (í•™ìŠµìš©)
        self.prev_state = None
        self.prev_action = None

        logger.info("ğŸš€ ì»¤ë§¨ë“œ ì„¼í„° ì´ˆê¸°í™” ì™„ë£Œ")

    def analyze_situation(self, candidates: List[Dict]) -> Dict:
        """
        ì „ì²´ ìƒí™© ë¶„ì„

        Args:
            candidates: ë§¤ìˆ˜ í›„ë³´ ì¢…ëª© ë¦¬ìŠ¤íŠ¸

        Returns:
            ìƒí™© ë¶„ì„ ê²°ê³¼
        """
        logger.info("\n" + "ğŸ¯" * 30)
        logger.info("ì»¤ë§¨ë“œ ì„¼í„°: ìƒí™© ë¶„ì„ ì‹œì‘")
        logger.info("ğŸ¯" * 30 + "\n")

        # 1. ì‹œì¥ ìƒíƒœ ë²¡í„° ìƒì„±
        state = self.market_state.get_state_vector(candidates)

        # 2. ì‹œì¥ ìƒí™© ë¶„ë¥˜
        market_condition = self.market_state.classify_market_condition(state)

        # 3. ì‹œì¥ ë¶„ì„ ì¶œë ¥
        self.market_state.print_market_analysis(state)

        # 4. AI ì¶”ì²œ í–‰ë™
        recommendation = self.rl_agent.get_action_recommendation(state, market_condition)
        self.rl_agent.print_recommendation(recommendation)

        # 5. ê±°ë˜ í†µê³„
        self.trade_history.print_statistics(recent_trades=20)

        # í˜„ì¬ ìƒíƒœ ì €ì¥ (ë‹¤ìŒ í•™ìŠµì— ì‚¬ìš©)
        self.prev_state = state
        self.prev_action = recommendation['best_action_id']

        return {
            "state": state,
            "market_condition": market_condition,
            "recommendation": recommendation,
        }

    def should_trade(self, analysis: Dict) -> bool:
        """
        ê±°ë˜ ì‹¤í–‰ ì—¬ë¶€ ê²°ì •

        Args:
            analysis: ìƒí™© ë¶„ì„ ê²°ê³¼

        Returns:
            ê±°ë˜ ì‹¤í–‰ ì—¬ë¶€
        """
        best_action = analysis['recommendation']['best_action_id']

        # ë§¤ìˆ˜ í–‰ë™ (0, 1, 2)ì´ë©´ ê±°ë˜
        should_trade = best_action in [
            RLAgent.ACTION_BUY_AGGRESSIVE,
            RLAgent.ACTION_BUY_MODERATE,
            RLAgent.ACTION_BUY_CONSERVATIVE,
        ]

        if should_trade:
            logger.info(f"âœ… ì»¤ë§¨ë“œ ì„¼í„° íŒë‹¨: ê±°ë˜ ì‹¤í–‰ ({analysis['recommendation']['best_action']})")
        else:
            logger.info(f"â¸ï¸  ì»¤ë§¨ë“œ ì„¼í„° íŒë‹¨: ê±°ë˜ ë³´ë¥˜ ({analysis['recommendation']['best_action']})")

        return should_trade

    def get_position_sizing_factor(self, analysis: Dict) -> float:
        """
        í¬ì§€ì…˜ ì‚¬ì´ì¦ˆ ì¡°ì ˆ ê³„ìˆ˜ ê³„ì‚°

        Args:
            analysis: ìƒí™© ë¶„ì„ ê²°ê³¼

        Returns:
            ì¡°ì ˆ ê³„ìˆ˜ (0.5 ~ 1.5)
        """
        best_action = analysis['recommendation']['best_action_id']
        market_condition = analysis['market_condition']

        # ê¸°ë³¸ ê³„ìˆ˜
        factor = 1.0

        # í–‰ë™ì— ë”°ë¥¸ ì¡°ì ˆ
        if best_action == RLAgent.ACTION_BUY_AGGRESSIVE:
            factor = 1.5  # ê³µê²©ì  ë§¤ìˆ˜: 1.5ë°°
        elif best_action == RLAgent.ACTION_BUY_MODERATE:
            factor = 1.0  # ë³´í†µ ë§¤ìˆ˜: 1.0ë°°
        elif best_action == RLAgent.ACTION_BUY_CONSERVATIVE:
            factor = 0.5  # ë³´ìˆ˜ì  ë§¤ìˆ˜: 0.5ë°°

        # ì‹œì¥ ìƒí™©ì— ë”°ë¥¸ ì¶”ê°€ ì¡°ì ˆ
        if market_condition == "STRONG_BULL":
            factor *= 1.2  # ê°•ì„¸ì¥: 20% ì¦ê°€
        elif market_condition == "STRONG_BEAR":
            factor *= 0.7  # ì•½ì„¸ì¥: 30% ê°ì†Œ

        logger.info(f"ğŸ“Š í¬ì§€ì…˜ ì‚¬ì´ì¦ˆ ì¡°ì ˆ ê³„ìˆ˜: {factor:.2f}x")

        return factor

    def update_from_trade_result(self, profit_rate: float):
        """
        ê±°ë˜ ê²°ê³¼ë¡œë¶€í„° í•™ìŠµ

        Args:
            profit_rate: ìˆ˜ìµë¥ 
        """
        if self.prev_state is None or self.prev_action is None:
            logger.warning("âš ï¸  ì´ì „ ìƒíƒœ/í–‰ë™ ì •ë³´ ì—†ìŒ. í•™ìŠµ ë¶ˆê°€")
            return

        # í˜„ì¬ ìƒíƒœ (ê±°ë˜ í›„)
        stats = self.trade_history.get_statistics(recent_trades=5)
        current_state = self.market_state.get_state_vector([])  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ í˜„ì¬ í†µê³„ë§Œ ë°˜ì˜

        # ì‹œì¥ ìƒí™©
        market_condition = self.market_state.classify_market_condition(current_state)

        # ë³´ìƒ ê³„ì‚°
        reward = self.rl_agent.calculate_reward(
            action=self.prev_action,
            profit_rate=profit_rate,
            market_condition=market_condition
        )

        # Q-learning ì—…ë°ì´íŠ¸
        self.rl_agent.update_q_value(
            state=self.prev_state,
            action=self.prev_action,
            reward=reward,
            next_state=current_state
        )

        logger.info(f"âœ… ê±°ë˜ ê²°ê³¼ í•™ìŠµ ì™„ë£Œ (ìˆ˜ìµë¥ : {profit_rate * 100:+.2f}%, ë³´ìƒ: {reward:+.3f})")

    def print_dashboard(self):
        """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ì¶œë ¥"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š ì»¤ë§¨ë“œ ì„¼í„° ëŒ€ì‹œë³´ë“œ")
        logger.info("=" * 60)

        # ê±°ë˜ í†µê³„
        stats = self.trade_history.get_statistics()
        logger.info(f"\n[ê±°ë˜ í†µê³„]")
        logger.info(f"ì´ ê±°ë˜: {stats['total_trades']}ê±´")
        logger.info(f"ìŠ¹ë¥ : {stats['win_rate'] * 100:.2f}%")
        logger.info(f"í‰ê·  ìˆ˜ìµë¥ : {stats['avg_profit_rate']:+.2f}%")
        logger.info(f"ì´ ìˆ˜ìµ: {stats['total_profit']:,}ì›")

        # AI í•™ìŠµ ìƒíƒœ
        logger.info(f"\n[AI í•™ìŠµ ìƒíƒœ]")
        logger.info(f"ì´ ì—…ë°ì´íŠ¸ íšŸìˆ˜: {self.rl_agent.total_updates}íšŒ")
        logger.info(f"Q-í…Œì´ë¸” í¬ê¸°: {len(self.rl_agent.q_table)}ê°œ ìƒíƒœ")
        logger.info(f"íƒí—˜ í™•ë¥  (Îµ): {self.rl_agent.epsilon * 100:.1f}%")

        # ìµœê·¼ ê±°ë˜ ë‚´ì—­
        recent_trades = self.trade_history.get_recent_trades(count=5)
        if recent_trades:
            logger.info(f"\n[ìµœê·¼ ê±°ë˜ ë‚´ì—­]")
            for i, trade in enumerate(recent_trades[-5:], 1):
                logger.info(
                    f"{i}. {trade['stock_name']} | "
                    f"ìˆ˜ìµë¥ : {trade['profit_rate']:+.2f}% | "
                    f"ìˆ˜ìµ: {trade['profit']:+,}ì›"
                )

        logger.info("=" * 60 + "\n")
