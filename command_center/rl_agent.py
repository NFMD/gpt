"""
ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ëª¨ë“ˆ
Q-learning ê¸°ë°˜ìœ¼ë¡œ ë§¤ë§¤ ì˜ì‚¬ê²°ì •ì„ í•™ìŠµí•©ë‹ˆë‹¤.
"""
import logging
import json
import numpy as np
from pathlib import Path
from typing import Dict, Tuple


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RLAgent:
    """ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ (Q-Learning)"""

    # í–‰ë™ ì •ì˜
    ACTION_BUY_AGGRESSIVE = 0  # ì ê·¹ ë§¤ìˆ˜
    ACTION_BUY_MODERATE = 1  # ë³´í†µ ë§¤ìˆ˜
    ACTION_BUY_CONSERVATIVE = 2  # ë³´ìˆ˜ì  ë§¤ìˆ˜
    ACTION_HOLD = 3  # ëŒ€ê¸°
    ACTION_SELL = 4  # ë§¤ë„

    ACTION_NAMES = {
        0: "ì ê·¹ ë§¤ìˆ˜",
        1: "ë³´í†µ ë§¤ìˆ˜",
        2: "ë³´ìˆ˜ì  ë§¤ìˆ˜",
        3: "ëŒ€ê¸°",
        4: "ë§¤ë„",
    }

    def __init__(
        self,
        state_size: int = 10,
        n_actions: int = 5,
        learning_rate: float = 0.1,
        discount_factor: float = 0.95,
        epsilon: float = 0.1,
    ):
        """
        Args:
            state_size: ìƒíƒœ ë²¡í„° ì°¨ì›
            n_actions: í–‰ë™ ê°œìˆ˜
            learning_rate: í•™ìŠµë¥  (alpha)
            discount_factor: í• ì¸ ê³„ìˆ˜ (gamma)
            epsilon: íƒí—˜ í™•ë¥ 
        """
        self.state_size = state_size
        self.n_actions = n_actions
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon

        # Q-í…Œì´ë¸” (ì´ì‚°í™”ëœ ìƒíƒœ ê³µê°„ ì‚¬ìš©)
        self.q_table_file = Path(__file__).parent.parent / "data" / "q_table.json"
        self.q_table = self._load_q_table()

        # í•™ìŠµ í†µê³„
        self.total_updates = 0

    def _load_q_table(self) -> Dict:
        """Q-í…Œì´ë¸” ë¡œë“œ"""
        if self.q_table_file.exists():
            with open(self.q_table_file, 'r') as f:
                return json.load(f)
        return {}

    def _save_q_table(self):
        """Q-í…Œì´ë¸” ì €ì¥"""
        self.q_table_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.q_table_file, 'w') as f:
            json.dump(self.q_table, f, indent=2)

    def _discretize_state(self, state: np.ndarray, bins: int = 5) -> str:
        """
        ì—°ì† ìƒíƒœë¥¼ ì´ì‚° ìƒíƒœë¡œ ë³€í™˜

        Args:
            state: ì—°ì† ìƒíƒœ ë²¡í„°
            bins: ê° ì°¨ì›ì„ ë‚˜ëˆŒ êµ¬ê°„ ìˆ˜

        Returns:
            ì´ì‚°í™”ëœ ìƒíƒœ ë¬¸ìì—´
        """
        discretized = []
        for value in state:
            # 0~1 ë²”ìœ„ë¥¼ binsê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ”
            bin_idx = min(int(value * bins), bins - 1)
            discretized.append(str(bin_idx))

        return "_".join(discretized)

    def get_q_values(self, state: np.ndarray) -> np.ndarray:
        """
        ì£¼ì–´ì§„ ìƒíƒœì—ì„œì˜ Qê°’ë“¤ ì¡°íšŒ

        Args:
            state: ìƒíƒœ ë²¡í„°

        Returns:
            ê° í–‰ë™ì— ëŒ€í•œ Qê°’ ë°°ì—´
        """
        state_key = self._discretize_state(state)

        if state_key not in self.q_table:
            # ì´ˆê¸° Qê°’: 0ìœ¼ë¡œ ì„¤ì •
            self.q_table[state_key] = [0.0] * self.n_actions

        return np.array(self.q_table[state_key])

    def select_action(self, state: np.ndarray, greedy: bool = False) -> int:
        """
        í–‰ë™ ì„ íƒ (Îµ-greedy ì •ì±…)

        Args:
            state: í˜„ì¬ ìƒíƒœ
            greedy: Trueë©´ ë¬´ì¡°ê±´ greedy ì„ íƒ (íƒí—˜ ì—†ìŒ)

        Returns:
            ì„ íƒëœ í–‰ë™ ì¸ë±ìŠ¤
        """
        if not greedy and np.random.random() < self.epsilon:
            # íƒí—˜: ë¬´ì‘ìœ„ í–‰ë™
            action = np.random.randint(self.n_actions)
            logger.debug(f"ğŸ² íƒí—˜: {self.ACTION_NAMES[action]}")
        else:
            # í™œìš©: ìµœì„ ì˜ í–‰ë™
            q_values = self.get_q_values(state)
            action = int(np.argmax(q_values))
            logger.debug(f"ğŸ¯ í™œìš©: {self.ACTION_NAMES[action]} (Q={q_values[action]:.3f})")

        return action

    def update_q_value(
        self,
        state: np.ndarray,
        action: int,
        reward: float,
        next_state: np.ndarray
    ):
        """
        Q-learning ì—…ë°ì´íŠ¸

        Q(s, a) â† Q(s, a) + Î±[r + Î³ max Q(s', a') - Q(s, a)]

        Args:
            state: í˜„ì¬ ìƒíƒœ
            action: ì‹¤í–‰í•œ í–‰ë™
            reward: ë°›ì€ ë³´ìƒ
            next_state: ë‹¤ìŒ ìƒíƒœ
        """
        state_key = self._discretize_state(state)

        # í˜„ì¬ Qê°’
        current_q = self.get_q_values(state)[action]

        # ë‹¤ìŒ ìƒíƒœì—ì„œì˜ ìµœëŒ€ Qê°’
        next_q_values = self.get_q_values(next_state)
        max_next_q = np.max(next_q_values)

        # TD íƒ€ê²Ÿ
        td_target = reward + self.gamma * max_next_q

        # TD ì˜¤ì°¨
        td_error = td_target - current_q

        # Qê°’ ì—…ë°ì´íŠ¸
        new_q = current_q + self.lr * td_error

        self.q_table[state_key][action] = new_q
        self.total_updates += 1

        # ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥
        if self.total_updates % 10 == 0:
            self._save_q_table()

        logger.info(
            f"ğŸ“š Q-learning ì—…ë°ì´íŠ¸: {self.ACTION_NAMES[action]}\n"
            f"   ë³´ìƒ: {reward:+.3f} | TD ì˜¤ì°¨: {td_error:+.3f} | "
            f"ìƒˆ Qê°’: {new_q:.3f}"
        )

    def calculate_reward(
        self,
        action: int,
        profit_rate: float,
        market_condition: str
    ) -> float:
        """
        ë³´ìƒ ê³„ì‚°

        Args:
            action: ì‹¤í–‰í•œ í–‰ë™
            profit_rate: ìˆ˜ìµë¥ 
            market_condition: ì‹œì¥ ìƒí™©

        Returns:
            ë³´ìƒê°’
        """
        # ê¸°ë³¸ ë³´ìƒ: ìˆ˜ìµë¥  ê¸°ë°˜
        reward = profit_rate

        # í–‰ë™ë³„ ë³´ë„ˆìŠ¤/í˜ë„í‹°
        if action in [self.ACTION_BUY_AGGRESSIVE, self.ACTION_BUY_MODERATE, self.ACTION_BUY_CONSERVATIVE]:
            # ë§¤ìˆ˜ í–‰ë™
            if profit_rate > 0.03:  # 3% ì´ìƒ ìˆ˜ìµ
                reward += 0.5  # ë³´ë„ˆìŠ¤
            elif profit_rate < -0.02:  # 2% ì´ìƒ ì†ì‹¤
                reward -= 0.5  # í˜ë„í‹°

            # ì‹œì¥ ìƒí™©ì— ë”°ë¥¸ ì¡°ì •
            if market_condition in ["STRONG_BULL", "BULL"]:
                reward += 0.2  # ìƒìŠ¹ì¥ì—ì„œ ë§¤ìˆ˜ëŠ” ì¢‹ì€ ì„ íƒ
            elif market_condition in ["STRONG_BEAR", "BEAR"]:
                reward -= 0.2  # í•˜ë½ì¥ì—ì„œ ë§¤ìˆ˜ëŠ” ë‚˜ìœ ì„ íƒ

        elif action == self.ACTION_HOLD:
            # ëŒ€ê¸° í–‰ë™
            if market_condition == "NEUTRAL":
                reward += 0.1  # ì¤‘ë¦½ì¥ì—ì„œ ëŒ€ê¸°ëŠ” í•©ë¦¬ì 
            else:
                reward -= 0.1  # ê¸°íšŒ ë¹„ìš©

        elif action == self.ACTION_SELL:
            # ë§¤ë„ í–‰ë™
            if profit_rate > 0:
                reward += 0.3  # ì´ìµ ì‹¤í˜„ ë³´ë„ˆìŠ¤
            else:
                reward -= 0.1  # ì†ì‹¤ ë§¤ë„ í˜ë„í‹°

        return reward

    def get_action_recommendation(
        self,
        state: np.ndarray,
        market_condition: str
    ) -> Dict:
        """
        í˜„ì¬ ìƒíƒœì—ì„œì˜ í–‰ë™ ì¶”ì²œ

        Args:
            state: í˜„ì¬ ìƒíƒœ
            market_condition: ì‹œì¥ ìƒí™©

        Returns:
            ì¶”ì²œ ì •ë³´
        """
        q_values = self.get_q_values(state)
        best_action = int(np.argmax(q_values))
        best_q_value = q_values[best_action]

        # í–‰ë™ë³„ Qê°’ ì •ë ¬
        sorted_indices = np.argsort(q_values)[::-1]

        recommendations = []
        for idx in sorted_indices[:3]:  # ìƒìœ„ 3ê°œ
            recommendations.append({
                "action": self.ACTION_NAMES[idx],
                "q_value": float(q_values[idx]),
            })

        return {
            "best_action": self.ACTION_NAMES[best_action],
            "best_action_id": best_action,
            "best_q_value": float(best_q_value),
            "market_condition": market_condition,
            "all_recommendations": recommendations,
        }

    def print_recommendation(self, recommendation: Dict):
        """ì¶”ì²œ ì •ë³´ ì¶œë ¥"""
        logger.info("=" * 60)
        logger.info("ğŸ¤– AI ì¶”ì²œ í–‰ë™")
        logger.info("=" * 60)
        logger.info(f"ì‹œì¥ ìƒí™©: {recommendation['market_condition']}")
        logger.info(f"â¡ï¸  ìµœì  í–‰ë™: {recommendation['best_action']} (Q={recommendation['best_q_value']:.3f})")
        logger.info("\nìƒìœ„ ì¶”ì²œ:")
        for i, rec in enumerate(recommendation['all_recommendations'], 1):
            logger.info(f"  {i}. {rec['action']:20s} (Q={rec['q_value']:.3f})")
        logger.info("=" * 60)
